{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n",
    "It is performed on continuous variables\n",
    "\n",
    "We have to predict the house prices base on 2 features:\n",
    "House sizes (feet2)\n",
    "Number of bedrooms in the house\n",
    "And we relized that house sizes are about 1000 times  the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n",
    "\n",
    "\n",
    "For e.g you want to see how speed of running of an athlete depends on his weight and height. Here the factors will be height and weight. Height ranges from say 4 feet to 7 feet while weight ranges from say 50 kgs to 120 kgs. If we feed in these factors in the model as it is then the model will give a higher weightage to weight as compared to height because the values for weights are larger than heights. This may not give us desired results. To overcome this, we would have to rescale or standardise the two features so that their range is comparable. And these values will now be fed to the model.\n",
    "\n",
    "For example, the age of employees in a company may be between 21-70 years, the size of the house they live is 500-5000 Sq feet and their salaries may range from $30000-$80000. In this situation if you use a simple Euclidean metric, the age feature will not play any role because it is several order smaller than other features. However, it may contain some important information that may be useful for the task. Here, you may want to normalize the features independently to the same scale, say [0,1], so they contribute equally while computing the distance. However, normalization may also result in loss of information. Therefore, you need to be sure about this aspect as well. Most of the time, it helps when the objective function you are optimizing computes some sort of distance or squared distance. \n",
    "\n",
    "\n",
    "In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i >= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).\n",
    "\n",
    "Some examples of algorithms where feature scaling matters are:\n",
    "\n",
    "- k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\n",
    "\n",
    "- k-means\n",
    "\n",
    "- logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\n",
    "\n",
    "\n",
    "\n",
    "List of functions used for Feature Scaling\n",
    "---------------------------------------------\n",
    "Zscore: Converts all values to a z-score.\n",
    "\n",
    "MinMax: The min-max normalizer linearly rescales every feature to the [0,1] interval.\n",
    "\n",
    "Rescaling to the [0,1] interval is done by shifting the values of each feature so that the minimal value is 0, and then dividing by the new maximal value (which is the difference between the original maximal and minimal values).\n",
    "\n",
    "Logistic: \n",
    "\n",
    "LogNormal: This option converts all values to a lognormal scale.\n",
    "\n",
    "TanH: All values are converted to a hyperbolic tangent.\n",
    "\n",
    "\n",
    "Normalization:\n",
    "x_new=(x-x_min)/(x_max-x_min )\n",
    "\n",
    "For normalization, the maximum number we can get after applying the formula is 1, and the minimum number is 0. So here is one big characteristic all the numbers will be between 0 and 1.\n",
    "\n",
    "Standardization:\n",
    "x_new=(x-μ)/σ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
