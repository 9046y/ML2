{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KNN algorithm is one of the simplest classification algorithm \n",
    "\n",
    "KNN is a non-parametric\n",
    "\n",
    "KNN is also a lazy algorithm (as opposed to an eager algorithm). What this means is that it does not use the training data points to do any generalization. In other words, there is no explicit training phase. Lack of generalization means that KNN keeps all the training data. To be more exact, all (or most) the training data is needed during the testing phase.\n",
    " \n",
    "Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. \n",
    "\n",
    "To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.\n",
    "\n",
    "Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.\n",
    "\n",
    "EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )\n",
    "\n",
    "\n",
    "Other popular distance measures include:\n",
    "\n",
    "Hamming Distance: Calculate the distance between binary vectors.\n",
    "Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance .\n",
    "Minkowski Distance: Generalization of Euclidean and Manhattan distance.\n",
    "K-Nearest Neighbors for Machine Learning\n",
    "by Jason Brownlee on April 15, 2016 in Machine Learning Algorithms\n",
    "In this post you will discover the k-Nearest Neighbors (KNN) algorithm for classification and regression. After reading this post you will know.\n",
    "\n",
    "The model representation used by KNN.\n",
    "How a model is learned using KNN (hint, it’s not).\n",
    "How to make predictions using KNN\n",
    "The many names for KNN including how different fields refer to it.\n",
    "How to prepare your data to get the most from KNN.\n",
    "Where to look to learn more about the KNN algorithm.\n",
    "This post was written for developers and assumes no background in statistics or mathematics. The focus is on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.\n",
    "\n",
    "Let’s get started.\n",
    "\n",
    "K-Nearest Neighbors for Machine Learning\n",
    "K-Nearest Neighbors for Machine Learning\n",
    "Photo by Valentin Ottone, some rights reserved.\n",
    "\n",
    "KNN Model Representation\n",
    "The model representation for KNN is the entire training dataset.\n",
    "\n",
    "It is as simple as that.\n",
    "\n",
    "KNN has no model other than storing the entire dataset, so there is no learning required.\n",
    "\n",
    "Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient.\n",
    "\n",
    "Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.\n",
    "\n",
    "Making Predictions with KNN\n",
    "KNN makes predictions using the training dataset directly.\n",
    "\n",
    "Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.\n",
    "\n",
    "To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.\n",
    "\n",
    "Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.\n",
    "\n",
    "EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )\n",
    "\n",
    "Other popular distance measures include:\n",
    "\n",
    "Hamming Distance: Calculate the distance between binary vectors (more).\n",
    "Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).\n",
    "Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).\n",
    "There are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. \n",
    "\n",
    "KNN for Regression\n",
    "When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.\n",
    "\n",
    "KNN for Classification\n",
    "When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.\n",
    "\n",
    "\n",
    "Curse of Dimensionality\n",
    "KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
